#lang gm-plateau-2017
@title[#:style 'unnumbered]{Appendix}

@figure-here["fig:badcode" @elem{Reticulated program @tt{myfile.py}}
@python|{
def make_strings()->List(String):
    xs = []
    for i in range(3):
        if   i == 0: xs.append(i)
        elif i == 1: xs.append(True)
        else       : xs.append(make_strings)
    return xs

def get_lengths(los:List(String))->List(Int):
    return [strlen(s) for s in los]

def strlen(s:String)->Int:
    return len(s)

strs = make_strings()
get_lengths(strs)
}|]

@figure["fig:errmsg" @elem{Reticulated's error message for @figure-ref{fig:badcode}}
@exact|{\footnotesize\raggedright\begin{verbatim}
Traceback (most recent call last):
  File "/.../retic", line 9, in <module>
    load_entry_point('retic==0.1.0',
                     'console_scripts',
		     'retic')()
  File "/.../retic/retic.py", line 155, in main
    reticulate(program,
               prog_args=args.args.split(),
	       flag_sets=args)
  File /.../retic/retic.py", line 104, in reticulate
    utils.handle_runtime_error(exit=True)
  File "/.../retic/retic.py", line 102, in reticulate
    _exec(code, __main__.__dict__)
  File "/.../retic/exec3/__init__.py", line 2, in _exec
    exec (obj, globs, locs)
  File "myfile.py", line 16, in <module>
    get_lengths(strs)
  File "myfile.py", line 10, in get_lengths
    return [strlen(s) for s in los]
  File "myfile.py", line 10, in <listcomp>
    return [strlen(s) for s in los]
  File "myfile.py", line 12, in strlen
    def strlen(s:String)->Int:
  File "/.../retic/runtime.py", line 109, in
      check_type_string
    return val if isinstance(val, str) else rse()
  File "/.../retic/runtime.py", line 88, in rse
    raise Exception(x)
Exception: None
\end{verbatim}}|]

@Figure-ref{fig:badcode} is a small Reticulated program that signals a type error at run-time.
The error message is reproduced in @figure-ref{fig:errmsg}.
It consists of a stack trace and the name of a failing check, as described in @section-ref{sec:vs-tr:errors}.
Notice, in particular, how the stack frames for @tt{myfile.py} guide developers to typed code.


@; ===

@section{Argument: SRA}

@Figure-ref{fig:sample:validate} demonstrates that a linear number of samples
 suffice to approximate the performance overhead in
 @integer->word[NUM-VALIDATE-SAMPLES] benchmarks from
 @section-ref{sec:exhaustive}.
For a benchmark containing @${F} functions and @${C} classes,
 the figure plots the confidence interval generated by
 @integer->word[NUM-SAMPLE-TRIALS] samples of @${@id[SAMPLE-RATE] * (F + C)}
 configurations selected at random @emph{with replacement}.@note{The theoretical
  justification in @section-ref{sec:sampling:protocol} assumes random sampling
  without replacement, but @; Knuth citation gratuitous
  the chance of drawing the same configuration twice is quite small, and removing
  this chance slightly increases the odds of drawing an extreme outlier.}
Each interval is superimposed on the matching overhead plot from @figure-ref{fig:overhead}.

These particular @approximation[NUM-SAMPLE-TRIALS (format "~a(F+C)" SAMPLE-RATE) "95"]s
 @; ... could just say "simple random approximation"
 all contain the the true number of @deliverable{D} configurations for values of
 @${D} between @${1} and @${@id[MAX-OVERHEAD]}.
The intervals are futhermore small, and thus practical substitutes for the overhead plots.

@figure*["fig:sample:validate" @elem{Validating the simple random approximation method}
  (parameterize ([*PLOT-HEIGHT* 100])
    @render-validate-samples-plot*[VALIDATE-BENCHMARKS])
]

To illustrate the protocol, suppose a few developers independently apply
 gradual typing to a program.
Each arrives at some configuration and observes some performance overhead.
For a given value of @${D} some proportion of the developers have
 @deliverable{D} configurations.
There is a remote chance that this proportion coincides with the true proportion
 of @deliverable{D} configurations.
Intuitively, the chance is less remote if the number of developers is large.
But even for a small number of developers, if they repeat this experiment
 multiple times, then the average proportion of @deliverable{D} configurations
 should tend towards the true proportion.
After all, if the true proportion of @deliverable{D} configurations is
 @${10\%} then approximately @${1} in @${10} randomly sampled configurations is
 @deliverable{D}.

The theoretical justification for why this protocol should yield a useful
 estimate requires some basic statistics.
Let @${d} be a predicate that checks whether a given configuration from
 a fixed program is @deliverable{D}.
This predicate defines a Bernoulli random variable @${X_d} with parameter
 @${p}, where @${p} is the true proportion of @deliverable{D} configurations.
Consequently, the expected value of this random variable is @${p}.
The law of large numbers therefore states that the average of infinitely
 many samples of @${X_d} converges to @${p}, the true proportion
 of deliverable configurations.
Convergence suggests that the average of ``enough'' samples is ``close to''
 @${p}.
The central limit theorem provides a similar guarantee---any sequence of
 such averages is normally distributed around the true proportion.
Hence a @${95\%} confidence interval generated from sample averages is likely
 to contain the true proportion.


@section{Benchmarks}

@(let ([total @integer->word[NUM-EXHAUSTIVE-BENCHMARKS]]
       [num1 @integer->word[(length DLS-2014-BENCHMARK-NAMES)]]
       [dls-names @authors*[(map (compose1 tt symbol->string) DLS-2014-BENCHMARK-NAMES)]]
       [num2 @integer->word[(length POPL-2017-BENCHMARK-NAMES)]]
       [num3 @integer->word[(length '(Espionage PythonFlow take5))]]
      ) @elem{
  @; Many of the benchmark programs stem from prior work on Reticulated.
  Of the @|total| benchmark programs,
   @|num1| originate from case studies by @citet[vksb-dls-2014],
   @;@note{@|dls-names|.}
   @|num2| are from the evaluation by @citet[vss-popl-2017] on programs from
   the Python Performance Benchmark Suite,
   and the remaining @|num3| originate from open-source programs.
  Every listing of the benchmarks in this section is ordered first by the
   benchmark's origin and second by the benchmark's name.
})
@; REMARK: original authors helpful with (code, test input, comments)

@(let* ([column-descr*
         (list
           @elem{lines of code (@bold{SLOC}), }
           @elem{number of modules (@bold{M}), }
           @elem{number of function and method definitions (@bold{F}), }
           @elem{and number of class definitions (@bold{C}).})]
        [num-col @integer->word[(length column-descr*)]]
       ) @elem{
  @Figure-ref{fig:static-benchmark} tabulates information about the size and
   structure of the @defn{experimental} portions of the benchmarks.
  The @|num-col| columns report the @|column-descr*|
})

The following descriptions credit each benchmark's original author,
 state whether it depends on any @defn{control} modules,
 and briefly summarize its purpose.


@; -----------------------------------------------------------------------------
@; --- WARNING: the order of benchmarks matters!
@; ---  Do not re-order without checking ALL PROSE in this file
@; -----------------------------------------------------------------------------

@bm-desc["futen"
@hyperlink["http://blog.amedama.jp/"]{@tt{momijiame}}
@url{https://github.com/momijiame/futen}
@list[
  @lib-desc["fnmatch"]{Filename matching}
  @lib-desc["os.path"]{Path split, path join, path expand, getenv}
  @lib-desc["re"]{One regular expression match}
  @lib-desc["shlex"]{Split host names from an input string}
  @lib-desc["socket"]{Basic socket operations}
]]{
  Converts an @hyperlink["https://www.openssh.com/"]{OpenSSH} configuration
  file to an inventory file for the
  @hyperlink["https://www.ansible.com/"]{@emph{Ansiable}} framework.
  @; 1900 iterations
}

@bm-desc["http2"
@authors[@hyperlink["https://github.com/httplib2/httplib2"]{Joe Gregorio}]
@url{https://github.com/httplib2/httplib2}
@list[
  @lib-desc["urllib"]{To split an IRI into components}
]]{
  Converts a collection of @hyperlink["https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier"]{Internationalized Resource Identifiers}
  to equivalent @hyperlink["http://www.asciitable.com/"]{ASCII} resource
  identifiers.
  @; 10 iterations
}

@bm-desc["slowSHA"
@authors["Stefano Palazzo"]
@url{http://github.com/sfstpala/SlowSHA}
@list[
  @lib-desc["os"]{path split}
]]{
  Applies the SHA-1 and SHA-512 algorithms to English words.
  @; 1 iteration
}

@bm-desc["call_method"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Microbenchmarks simple method calls;
  the calls do not use argument lists,
  keyword arguments, or tuple unpacking.
  @; Consists of @${32*10^5} calls to trivial functions.
  @; 1 iteration
}

@bm-desc["call_method_slots"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Same as @bm{call_method}, but using receiver objects that declare their methods
   in their @hyperlink["https://docs.python.org/3/reference/datamodel.html#slots"]{@tt{__slots__}}
   attribute.
  @; 1 iteration
}

@bm-desc["call_simple"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Same as @bm{call_method}, using functions rather than methods.
}

@bm-desc["chaos"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[
  @lib-desc["math"]{Square root}
  @lib-desc["random"]{randrange}
]]{
  Creates fractals using the @hyperlink["https://en.wikipedia.org/wiki/Chaos_game"]{@emph{chaos game}} method.
  @; 1 iteration
}

@bm-desc["fannkuch"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Implements Anderson and Rettig's microbenchmark@~cite[ar-lp-1994].
  @; 1 iteration
}

@bm-desc["float"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[
  @lib-desc["math"]{Sin, Cos, Sqrt}
]]{
  Microbenchmarks floating-point operations.
  @; 1 iteration (200,000 points)
}

@bm-desc["go"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[
  @lib-desc["math"]{sqrt log}
  @lib-desc["random"]{randrange random}
  "two untyped modules"
]]{
  Implements the game @hyperlink["https://en.wikipedia.org/wiki/Go_(game)"]{Go}.
  This benchmark is split across three files: an @defn{experimental} module that implements
  the game board, a @defn{control} module that defines constants, and a @defn{control} module
  that implements an AI and drives the benchmark.
  @; 2 iterations
}

@bm-desc["meteor"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Solves the Shootout benchmarks meteor puzzle.
  @note{@url{http://benchmarksgame.alioth.debian.org/u32/meteor-description.html}}
  @; 1 iterations (finds at most 6,000 solutions)
}

@bm-desc["nbody"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Models the orbits of the @hyperlink["https://en.wikipedia.org/wiki/Giant_planet"]{Jovian planets}.
  @; 1 iteration
}

@bm-desc["nqueens"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Solves the @hyperlink["https://developers.google.com/optimization/puzzles/queens"]{@math{N} queens} problem by a brute-force algorithm.
  @; 10 iterations
}

@bm-desc["pidigits"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Microbenchmarks big-integer arithmetic.
  @; 1 iteration (5,000 digits)
}

@bm-desc["pystone"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Implements Weicker's @emph{Dhrystone} benchmark.
  @note{@url{http://www.eembc.org/techlit/datasheets/ECLDhrystoneWhitePaper2.pdf}}
  @; 50,000 iterations
}

@bm-desc["spectralnorm"
@authors["The Python Benchmark Suite"]
@url{https://github.com/python/performance}
@list[]]{
  Computes the largest singular value of an infinite matrix.
  @; 10 iterations
}

@bm-desc["Espionage"
@authors["Zeina Migeed"]
""
@list[
  @lib-desc["operator"]{itemgetter}
]]{
  Implements Kruskal's spanning-tree algorithm.
  @; 1 iteration
}

@bm-desc["PythonFlow"
@authors["Alfian Ramadhan"]
@url{https://github.com/masphei/PythonFlow}
@list[
  @lib-desc["os"]{path join}
]]{
  Implements the Ford-Fulkerson max flow algorithm. 
  @; no longer needs citation
  @;@~cite[ff-cjm-1956].
  @; 1 iteration
}

@bm-desc["take5"
@authors["Maha Alkhairy" "Zeina Migeed"]
""
@list[
  @lib-desc["random"]{randrange shuffle random seed}
  @lib-desc["copy"]{deepcopy}
]]{
  Implements a card game and a simple player AI.
  @; 500 iterations
}

@bm-desc["sample_fsm"
@authors["Linh Chi Nguyen"]
@url{https://github.com/ayaderaghul/sample-fsm}
@list[
  @lib-desc["itertools"]{cycles}
  @lib-desc["os"]{path split}
  @lib-desc["random"]{random randrange}
]]{
  Simulates the interactions of economic agents via finite-state automata@~cite[n-mthesis-2014].
  This benchmark is adapted from a similar Racket program called @tt{fsmoo}@~cite[greenman-jfp-2017].
  @; 100 iterations
}

@bm-desc["aespython"
@authors[@hyperlink["http://caller9.com/"]{Adam Newman}
         @hyperlink["https://github.com/serprex"]{Demur Remud}]
@url{https://github.com/serprex/pythonaes}
@list[
  @lib-desc["os"]{random stat}
  @lib-desc["struct"]{pack unpack calcsize}
]]{
  @; Second sentence is a little awkward. I just want to say, "this is really
  @;  a Python implementation of AES, not just a wrapper to some UNIX implementation"
  Implements the @hyperlink["http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf"]{Advanced Encryption Standard}.
  Uses the @tt{os} library only to generate random bytes and invoke the
   @hyperlink["http://man7.org/linux/man-pages/man2/stat.2.html"]{@tt{stat()}}
   system call.
  @; 1 iteration, encrypts the book of Leviticus (2800 lines)
}

@bm-desc["stats"
@authors[@hyperlink["https://connects.catalyst.harvard.edu/Profiles/display/Person/12467"]{Gary Strangman}]
@url{https://github.com/seperman/python-statlib/blob/master/statlib/pstat.py}
@list[
  @lib-desc["copy"]{deepcopy}
  @lib-desc["math"]{pow abs etc.}
]]{
  Implements first-order statistics functions; in other words, transformations
   on either floats or (possibly-nested) lists of floats.
  The original program consists of two modules.
  The benchmark is modularized according to comments in the program's source
   code to reduce the size of each module's configuration space.
  @; 1 iteration
}

@; 

@section{Evaluation: More Details}

Three details of the Karst protocol warrant further attention.
First, nodes selected a random configuration by reading from a
 text file that contained a permutation of the configuration space.
This text file was stored on a dedicated machine.
Second, the same dedicated machine that stored the text file also stored
 all configurations for each @emph{module} of each benchmark.
After a node selected a configuration to run, it copied the relevant files
 to private storage before running the main module.
Third, we wrapped the main computation of every benchmark in a
 @hyperlink["https://www.python.org/dev/peps/pep-0318/"]{@tt{with} statement}
 that recorded execution time using the Python function @|time.process_time|.

